{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.fasttext import FastText as ft\n",
    "from numpy import genfromtxt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "             \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "             \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\",\n",
    "             \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\",\n",
    "             \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\",\n",
    "             \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\",\n",
    "             \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
    "             \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
    "             \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\n",
    "             \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "\n",
    "def load_embeddings(words):\n",
    "    en_model = ft.load_fasttext_format('drug-reviews/wiki/wiki.en.bin')\n",
    "    embedding = np.array([])\n",
    "    for word in words:\n",
    "        embedding = np.append(embedding, en_model[word[0]])\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def top_words(lines, num_words, n_grams):\n",
    "    all_tokens = []\n",
    "    for line in lines:\n",
    "        tokens = tokenize(line, n_grams)\n",
    "        for t in tokens:\n",
    "            if t not in stopwords:\n",
    "                all_tokens.append(t)\n",
    "    counter = Counter(all_tokens)\n",
    "    return counter.most_common(num_words)\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text,n_grams):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return [' '.join(x) for x in ngrams(tokens, n_grams)]\n",
    "  \n",
    "\n",
    "\n",
    "def ngrams(tokens, n):\n",
    "    output = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        output.append(tokens[i:i + n])\n",
    "    return output\n",
    "\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne-bi.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "    plt.savefig(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main    \n",
    "# read raw data \n",
    "raw_data = pd.read_csv('full_merge.csv')\n",
    "\n",
    "# extract review lines\n",
    "reviews = raw_data['Reviews']\n",
    "#print(reviews[3])\n",
    "# process and extract top n-grams\n",
    "top_words = top_words(reviews, 100, 2)\n",
    "print(top_words)\n",
    "with open('drug-reviews/top_words_n-grams.txt', mode='w', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join([w[0] for w in top_words]))\n",
    "\n",
    "# for words in our vocabulary we load the pre-trained embeddings from wikipedia data provided by fasttext      \n",
    "embeddings = load_embeddings(top_words)\n",
    "np.savetxt('drug-reviews/bigram-embeddings', embeddings, delimiter=',')\n",
    "top_words = []\n",
    "with open('drug-reviews/top_words_bigram.txt', mode='r', encoding='utf-8') as myfile:\n",
    "    top_words = myfile.readlines()\n",
    "embeddings = genfromtxt('drug-reviews/bigram-embeddings')\n",
    "embeddings = embeddings.reshape(100, 300)\n",
    "print(np.shape(embeddings))\n",
    "\n",
    "# apply t-SNE\n",
    "tsne = TSNE(perplexity=10.0, n_components=2, init='pca', n_iter=5000)\n",
    "low_dim_embedding = tsne.fit_transform(embeddings)\n",
    "\n",
    "#plot\n",
    "plot_with_labels(low_dim_embedding, top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
